{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\anaconda\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\anaconda\\lib\\site-packages (from scrapy) (2.7)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\anaconda\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\anaconda\\lib\\site-packages (from scrapy) (1.21.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in c:\\anaconda\\lib\\site-packages (from scrapy) (4.7.1)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in c:\\anaconda\\lib\\site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: Twisted>=17.9.0; python_version >= \"3.5\" in c:\\anaconda\\lib\\site-packages (from scrapy) (19.10.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\anaconda\\lib\\site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\anaconda\\lib\\site-packages (from scrapy) (1.12.0)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\anaconda\\lib\\site-packages (from scrapy) (19.0.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\anaconda\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\anaconda\\lib\\site-packages (from scrapy) (1.5.2)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\anaconda\\lib\\site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\anaconda\\lib\\site-packages (from scrapy) (4.3.4)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in c:\\anaconda\\lib\\site-packages (from cryptography>=2.0->scrapy) (0.24.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\anaconda\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.12.3)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from zope.interface>=4.1.3->scrapy) (41.0.1)\n",
      "Requirement already satisfied: attrs>=16.0.0 in c:\\anaconda\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (19.1.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\anaconda\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\anaconda\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\anaconda\\lib\\site-packages (from Twisted>=17.9.0; python_version >= \"3.5\"->scrapy) (15.1.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in c:\\anaconda\\lib\\site-packages (from Twisted>=17.9.0; python_version >= \"3.5\"->scrapy) (17.5.0)\n",
      "Requirement already satisfied: Automat>=0.3.0 in c:\\anaconda\\lib\\site-packages (from Twisted>=17.9.0; python_version >= \"3.5\"->scrapy) (0.8.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\anaconda\\lib\\site-packages (from Twisted>=17.9.0; python_version >= \"3.5\"->scrapy) (19.0.0)\n",
      "Requirement already satisfied: PyHamcrest>=1.9.0 in c:\\anaconda\\lib\\site-packages (from Twisted>=17.9.0; python_version >= \"3.5\"->scrapy) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\anaconda\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.19)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\anaconda\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0; python_version >= \"3.5\"->scrapy) (2.8)\n",
      "First 100 links extracted!\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import pandas as pd\n",
    "\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = \"WS\"\n",
    "    \n",
    "    # Here is where the API call is inserted\n",
    "    # start_urls = [\n",
    "        #'https://projecta.com/about/our-team/'\n",
    "        #]\n",
    "        \n",
    "    start_urls = [\n",
    "        'https://mobile.twitter.com/hashtag/cats'\n",
    "        ]\n",
    "        \n",
    "        \n",
    "\n",
    "    # Identifying the desired information from the query response and using xpath to extract\n",
    "    \n",
    "    def parse(self, response):\n",
    "        tweets = response.xpath('//table[@class=\"tweet  \"]/@href').getall()    \n",
    "        logging.info(f'{len(tweets)} tweets found')    \n",
    "        for tweet_id in tweets:        \n",
    "            tweet_id = re.findall(\"\\d+\", tweet_id)[-1]        \n",
    "            tweet_url = 'https://twitter.com/anyuser/status/'+str(tweet_id)           \n",
    "            yield scrapy.Request(tweet_url, callback=self.parse_tweet)\n",
    "            \n",
    "        # for item in response.xpath('//lh'):\n",
    "            # The ns code identifies the type of page the link comes from, and '0' means it is a Wikipedia entry\n",
    "            # Other codes indicate links from 'Talk' pages, etc.  Only entries are of interest here, so filtering is done\n",
    "            # if item.xpath('@ns').extract_first() == '0':\n",
    "                #yield {\n",
    "                    #'title': item.xpath('@title').extract_first() }\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                    \n",
    "        # Getting the information required to continue to the next ten entries\n",
    "        # next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        # Recursively calling the spider to process the next ten entries, if they exist\n",
    "        # if next_page is not None:\n",
    "            #next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            #yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "            \n",
    "        next_page = response.xpath('//*[@class=\"w-button-more\"]/a/@href').get(default='')\n",
    "        logging.info('Next page found:')\n",
    "        if next_page != '':\n",
    "            next_page = 'https://mobile.twitter.com' + next_page\n",
    "            yield scrapy.Request(next_page, callback=self.find_tweets)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'PythonLinks.json',\n",
    "    \n",
    "    \n",
    "     # Note that because the API queries are being done, the robots.txt file doesn't apply\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # Use CLOSESPIDER_PAGECOUNT to limit the scraper to the first 100 links    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(WikiSpider)\n",
    "process.start()\n",
    "print('First 100 links extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "Monty=pd.read_json('PythonLinks.json', lines=True)\n",
    "print(Monty.shape)\n",
    "print(Monty.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
